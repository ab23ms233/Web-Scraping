{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c6125d",
   "metadata": {},
   "source": [
    "Importing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea4e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import json\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d8a285",
   "metadata": {},
   "source": [
    "SCRAPING QUOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa0a55a",
   "metadata": {},
   "source": [
    "Initialising variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e32445",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://quotes.toscrape.com/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a034dab",
   "metadata": {},
   "source": [
    "Scraping a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97755383",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = soup.find_all(\"div\", class_=\"quote\")\n",
    "\n",
    "for quote in quotes:\n",
    "    text = quote.find(\"span\", class_=\"text\").get_text(strip=True)\n",
    "    author = quote.find(\"small\", class_=\"author\").get_text(strip=True)\n",
    "    tags = [tag.get_text(strip=True) for tag in quote.find_all(\"a\", class_=\"tag\")]\n",
    "\n",
    "    for tag in tags:\n",
    "        data[author][tag].append(text)\n",
    "\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a012d3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/page/2/'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_button = soup.find(\"li\", class_=\"next\")\n",
    "next_href = next_button.find(\"a\")\n",
    "next_href[\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a196c3c",
   "metadata": {},
   "source": [
    "Scraping Multiple Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b53824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Albert Einstein\n",
      "J.K. Rowling\n",
      "Jane Austen\n",
      "Marilyn Monroe\n",
      "Andr√© Gide\n",
      "Thomas A. Edison\n",
      "Eleanor Roosevelt\n",
      "Steve Martin\n",
      "\n",
      "2\n",
      "Bob Marley\n",
      "Dr. Seuss\n",
      "Douglas Adams\n",
      "Elie Wiesel\n",
      "Friedrich Nietzsche\n",
      "Mark Twain\n",
      "Allen Saunders\n",
      "\n",
      "3\n",
      "Pablo Neruda\n",
      "Ralph Waldo Emerson\n",
      "Mother Teresa\n",
      "Garrison Keillor\n",
      "Jim Henson\n",
      "\n",
      "4\n",
      "Charles M. Schulz\n",
      "William Nicholson\n",
      "Jorge Luis Borges\n",
      "George Eliot\n",
      "\n",
      "5\n",
      "George R.R. Martin\n",
      "C.S. Lewis\n",
      "Martin Luther King Jr.\n",
      "James Baldwin\n",
      "\n",
      "6\n",
      "Haruki Murakami\n",
      "Alexandre Dumas fils\n",
      "Stephenie Meyer\n",
      "Ernest Hemingway\n",
      "Helen Keller\n",
      "George Bernard Shaw\n",
      "\n",
      "7\n",
      "Charles Bukowski\n",
      "Suzanne Collins\n",
      "J.R.R. Tolkien\n",
      "\n",
      "8\n",
      "Alfred Tennyson\n",
      "Terry Pratchett\n",
      "J.D. Salinger\n",
      "George Carlin\n",
      "John Lennon\n",
      "W.C. Fields\n",
      "Ayn Rand\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "Jimi Hendrix\n",
      "J.M. Barrie\n",
      "E.E. Cummings\n",
      "Khaled Hosseini\n",
      "Harper Lee\n",
      "Madeleine L'Engle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://quotes.toscrape.com/\"   \n",
    "url = base_url\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36\"}      # Chrome browser string\n",
    "data = defaultdict(lambda: defaultdict(list))       # Quote data is stored here\n",
    "author_details = defaultdict()      # Author details are stored here\n",
    "page_count = 0\n",
    "\n",
    "while url:\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=header)     # Getting response from website\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    quotes = soup.find_all(\"div\", class_=\"quote\")       # Find all quotes in 1 page\n",
    "    \n",
    "    page_count += 1\n",
    "    print(page_count)\n",
    "\n",
    "    for quote in quotes:\n",
    "        text = quote.find(\"span\", class_=\"text\").get_text(strip=True)       # quote_text\n",
    "        author = quote.find(\"small\", class_=\"author\").get_text(strip=True)      # author\n",
    "        tags = [tag.get_text(strip=True) for tag in quote.find_all(\"a\", class_=\"tag\")]      # tags associated with the quote\n",
    "\n",
    "        for tag in tags:\n",
    "            data[author][tag].append(text)      # Listing all quotes by author and tag\n",
    "\n",
    "        if author not in author_details:        # Scraping author details if not scraped\n",
    "            print(author)\n",
    "            about_href = quote.find(\"a\")[\"href\"]\n",
    "            author_url = base_url + about_href\n",
    "\n",
    "            author_response = requests.get(author_url)\n",
    "            author_soup = BeautifulSoup(author_response.text, \"html.parser\")\n",
    "\n",
    "            born_date = author_soup.find(\"span\", class_=\"author-born-date\").get_text(strip=True)\n",
    "            born_location = author_soup.find(\"span\", class_=\"author-born-location\").get_text(strip=True)[3:]\n",
    "\n",
    "            author_details[author] = {\"Born On\": born_date, \"Location\": born_location}\n",
    "            time.sleep(random.uniform(1, 3))        # Delay requests to reduce traffic on website\n",
    "            \n",
    "    next_button = soup.find(\"li\", class_=\"next\")        # Next button at the end of page for author_details\n",
    "\n",
    "    if next_button:     # If next_buuton is availbale\n",
    "        next_href = next_button.find(\"a\")[\"href\"]\n",
    "        url = base_url + next_href      # url for next page\n",
    "    else:\n",
    "        url = None\n",
    "    \n",
    "    print()\n",
    "    time.sleep(random.uniform(1, 3))        # Delay requests to reduce traffic on website for next_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579cccd",
   "metadata": {},
   "source": [
    "Writing data to JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c1fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"quotes.json\", mode=\"w\", encoding='utf-8') as q:\n",
    "    json.dump(data,  q, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(\"author_details.json\", mode=\"w\", encoding='utf-8') as a:\n",
    "    json.dump(author_details, a, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ef96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
