{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c6125d",
   "metadata": {},
   "source": [
    "Importing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea4e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from colorama import Fore, init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d8a285",
   "metadata": {},
   "source": [
    "SCRAPING QUOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa0a55a",
   "metadata": {},
   "source": [
    "Initialising variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e32445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https://quotes.toscrape.com/\"   \n",
    "url = base_url\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36\"}      # Chrome browser string\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a034dab",
   "metadata": {},
   "source": [
    "Scraping a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97755383",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = soup.find_all(\"div\", class_=\"quote\")\n",
    "\n",
    "for quote in quotes:\n",
    "    text = quote.find(\"span\", class_=\"text\").get_text(strip=True)\n",
    "    author = quote.find(\"small\", class_=\"author\").get_text(strip=True)\n",
    "    tags = [tag.get_text(strip=True) for tag in quote.find_all(\"a\", class_=\"tag\")]\n",
    "\n",
    "    for tag in tags:\n",
    "        data[author][tag].append(text)\n",
    "\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a012d3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/page/2/'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_button = soup.find(\"li\", class_=\"next\")\n",
    "next_href = next_button.find(\"a\")\n",
    "next_href[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f98226c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoteScraping:\n",
    "    base_url = \"https://quotes.toscrape.com/\"\n",
    "    init(autoreset=True)\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.timeout = 5\n",
    "        self.session = requests.Session()\n",
    "        self.header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36\"}      # Chrome browser string\n",
    "        self.delay = [1.5, 3]\n",
    "\n",
    "    def author_list(self) -> List[str]:\n",
    "        url = QuoteScraping.base_url\n",
    "        author_set = set()\n",
    "        page_count = 0\n",
    "\n",
    "        while url:\n",
    "            try:\n",
    "                response = self.session.get(url, timeout=self.timeout, headers=self.header)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                raise Exception(f\"Error fetching {url}: {e}\")\n",
    "            \n",
    "            page_count += 1\n",
    "            print(f\"Scraping page {page_count}...\")\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            authors = soup.select(\"small.author\")\n",
    "            \n",
    "            author_set.update([author.get_text(strip=True) for author in authors])\n",
    "            next_button = soup.find(\"li\", class_=\"next\")\n",
    "\n",
    "            if next_button:\n",
    "                next_href = next_button.find(\"a\")[\"href\"]\n",
    "                url = QuoteScraping.base_url + next_href\n",
    "                time.sleep(random.uniform(self.delay[0], self.delay[1]))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return list(author_set)\n",
    "    \n",
    "    def scrape_author_quotes(self, author: str) -> Dict[str, List[str]]:\n",
    "        if not isinstance(author, str):\n",
    "            raise TypeError(\"Author name must be a string.\")\n",
    "        \n",
    "        page_count = 0\n",
    "        author_quotes = defaultdict()\n",
    "        author = author.lower()\n",
    "        url = QuoteScraping.base_url\n",
    "\n",
    "        while url:\n",
    "            try:\n",
    "                response = self.session.get(url, timeout=self.timeout, headers=self.header)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                raise Exception(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "            page_count += 1\n",
    "            print(Fore.GREEN + f\"Searching page {page_count}...\")\n",
    "            print()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            authors = soup.find_all(\"small\", class_=\"author\")\n",
    "\n",
    "            for auth in authors:\n",
    "                name = auth.get_text(strip=True).lower()\n",
    "\n",
    "                if name == author:\n",
    "                    quote = auth.find_parent(\"div\", class_=\"quote\")\n",
    "                    text = quote.select_one(\"span.text\").get_text(strip=True)\n",
    "                    tags = [tag.get_text(strip=True) for tag in quote.select(\"a.tag\")]\n",
    "\n",
    "                    print(Fore.MAGENTA + f\"ðŸ“œ Quote: {text}\")\n",
    "                    print(Fore.CYAN + f\"ðŸ·ï¸  Tags: {', '.join(tags)}\")\n",
    "                    print(\"-\" * 60)\n",
    "\n",
    "                    author_quotes[text] = tags\n",
    "            \n",
    "            print()\n",
    "            next_button = soup.find(\"li\", class_=\"next\")\n",
    "\n",
    "            if next_button:\n",
    "                next_href = next_button.find(\"a\")[\"href\"]\n",
    "                url = QuoteScraping.base_url + next_href\n",
    "                time.sleep(random.uniform(self.delay[0], self.delay[1]))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        if len(author_quotes) == 0:\n",
    "            raise ValueError(f\"No quotes found for author {author}.\")\n",
    "        \n",
    "        return dict(author_quotes)\n",
    "    \n",
    "    def scrape_author_info(self, author: str) -> Dict[str, str]:\n",
    "        if not isinstance(author, str):\n",
    "            raise TypeError(\"Author name must be a string.\")\n",
    "        \n",
    "        page_count = 0\n",
    "        author = author.lower()\n",
    "        url = QuoteScraping.base_url\n",
    "\n",
    "        while url:\n",
    "            try:\n",
    "                response = self.session.get(url, timeout=self.timeout, headers=self.header)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                raise Exception(f\"Error fetching {url}: {e}\")\n",
    "            \n",
    "            page_count += 1\n",
    "            print(Fore.GREEN + f\"Searching page {page_count}...\")\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")          \n",
    "            authors = soup.find_all(\"small\", class_=\"author\")\n",
    "\n",
    "            for auth in authors:\n",
    "                name = auth.get_text(strip=True)\n",
    "\n",
    "                if name.lower() == author:\n",
    "                    quote = auth.find_parent(\"div\", class_=\"quote\")\n",
    "                    about_href = quote.find(\"a\", class_=None)[\"href\"]\n",
    "                    author_url = QuoteScraping.base_url + about_href\n",
    "\n",
    "                    author_response = self.session.get(author_url, timeout=self.timeout, headers=self.header)\n",
    "                    author_soup = BeautifulSoup(author_response.text, \"html.parser\")\n",
    "\n",
    "                    born = author_soup.select_one(\"span.author-born-date\").get_text(strip=True)\n",
    "                    location = author_soup.select_one(\"span.author-born-location\").get_text(strip=True)\n",
    "                    description = author_soup.select_one(\"div.author-description\").get_text(strip=True)\n",
    "                    description = \".\".join(description.split('.', maxsplit=6)[:5])\n",
    "\n",
    "                    print()\n",
    "                    print(f\"ðŸ‘¤ Author: {name}\")\n",
    "                    print(f\"ðŸŽ‚ Born: {born}\")\n",
    "                    print(f\"ðŸ“ Location: {location}\")\n",
    "                    print(f\"ðŸ“ Bio: {description}\")\n",
    "                    print(\"-\" * 70)\n",
    "\n",
    "                    author_info = {\"Born\": born, \"Location\": location, \"Bio\": description}\n",
    "                    return author_info\n",
    "            \n",
    "            next_button = soup.find(\"li\", class_=\"next\")\n",
    "\n",
    "            if next_button:\n",
    "                next_href = next_button.find(\"a\")[\"href\"]\n",
    "                url = QuoteScraping.base_url + next_href\n",
    "                time.sleep(random.uniform(self.delay[0], self.delay[1]))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            raise ValueError(f\"Author {author} not found.\")\n",
    "    \n",
    "    def scrape_all_quotes(self) -> Dict[str, Dict[str, List[str]]]:\n",
    "        data = defaultdict(lambda: defaultdict(list))       # Quote data is stored here\n",
    "        page_count = 0\n",
    "        url = QuoteScraping.base_url\n",
    "\n",
    "        while url:\n",
    "            try:\n",
    "                response = self.session.get(url, timeout=self.timeout, headers=self.header)     # Getting response from website\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                raise Exception(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            quotes = soup.find_all(\"div\", class_=\"quote\")       # Find all quotes in 1 page\n",
    "\n",
    "            page_count += 1\n",
    "            print(f\"Scraping page {page_count}...\")\n",
    "\n",
    "            for quote in quotes:\n",
    "                text = quote.find(\"span\", class_=\"text\").get_text(strip=True)       # quote_text\n",
    "                author = quote.find(\"small\", class_=\"author\").get_text(strip=True)      # author\n",
    "                tags = [tag.get_text(strip=True) for tag in quote.find_all(\"a\", class_=\"tag\")]      # tags associated with the quote\n",
    "\n",
    "                for tag in tags:\n",
    "                    data[author][tag].append(text)      # Listing all quotes by author and tag\n",
    "\n",
    "            next_button = soup.find(\"li\", class_=\"next\")        # Next button at the end of page for author_details\n",
    "\n",
    "            if next_button:     # If next_buuton is availbale\n",
    "                next_href = next_button.find(\"a\")[\"href\"]\n",
    "                url = QuoteScraping.base_url + next_href      # url for next page\n",
    "                time.sleep(random.uniform(self.delay[0], self.delay[1]))        # Delay requests to reduce traffic on website for next_page\n",
    "            else:\n",
    "                url = None\n",
    "\n",
    "        return dict(data)\n",
    "\n",
    "    def scrape_all_authors(self) -> Dict[str, str]:\n",
    "        author_details = defaultdict()      # Author details are stored here\n",
    "        page_count = 0\n",
    "        url = QuoteScraping.base_url\n",
    "        \n",
    "        while url:\n",
    "            try:\n",
    "                response = requests.get(url, timeout=5, headers=self.header)     # Getting response from website\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                raise Exception(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            authors = soup.select(\"small.author\")\n",
    "\n",
    "            page_count += 1\n",
    "            print(Fore.GREEN + f\"Scraping page {page_count}...\")\n",
    "            print(Fore.CYAN + \"Reading authors: \")\n",
    "\n",
    "            for auth in authors:\n",
    "                name = auth.get_text(strip=True)      # author name\n",
    "\n",
    "                if name not in author_details:        # Scraping author details if not scraped\n",
    "                    print(name)\n",
    "                    quote = auth.find_parent(\"div\", class_=\"quote\") \n",
    "                    about_href = quote.find(\"a\", class_=None)[\"href\"]\n",
    "                    author_url = QuoteScraping.base_url + about_href\n",
    "\n",
    "                    author_response = requests.get(author_url)\n",
    "                    author_soup = BeautifulSoup(author_response.text, \"html.parser\")\n",
    "\n",
    "                    born = author_soup.find(\"span\", class_=\"author-born-date\").get_text(strip=True)\n",
    "                    location = author_soup.find(\"span\", class_=\"author-born-location\").get_text(strip=True)[3:]\n",
    "                    description = author_soup.select_one(\"div.author-description\").get_text(strip=True)\n",
    "\n",
    "                    author_details[name] = {\"Born\": born, \"Location\": location, \"Bio\": description}\n",
    "                    time.sleep(random.uniform(self.delay[0], self.delay[1]))        # Delay requests to reduce traffic on website\n",
    "\n",
    "            print()\n",
    "            next_button = soup.find(\"li\", class_=\"next\")        # Next button at the end of page for author_details\n",
    "\n",
    "            if next_button:     # If next_buuton is availbale\n",
    "                next_href = next_button.find(\"a\")[\"href\"]\n",
    "                url = QuoteScraping.base_url + next_href      # url for next page\n",
    "                time.sleep(random.uniform(self.delay[0], self.delay[1]))        # Delay requests to reduce traffic on website\n",
    "            else:\n",
    "                url = None\n",
    "\n",
    "        return dict(author_details)\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_to_json(data: Dict[str, Any], filename: str) -> None:\n",
    "        if not isinstance(filename, str):\n",
    "            raise TypeError(\"Filename must be a string\")\n",
    "        if not isinstance(data, dict):\n",
    "            raise TypeError(\"Data must be a dictionary\")\n",
    "        \n",
    "        with open(file=filename, mode='w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec888860",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookScraping(QuoteScraping):\n",
    "    base_url = \"https://books.toscrape.com/\"\n",
    "    rating_map = {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5}\n",
    "    init(autoreset=True)\n",
    "    \n",
    "    def genre_list(self) -> List[str]:\n",
    "        try:\n",
    "            response = self.session.get(url=BookScraping.base_url, timeout=self.timeout, headers=self.header)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"Error fetching {url}: {e}\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        side_panel = soup.find(\"ul\", class_=\"nav nav-list\")\n",
    "        genres = side_panel.select(\"ul ul li\")\n",
    "\n",
    "        genre_list = [genre.find(\"a\").get_text(strip=True) for genre in genres]\n",
    "        return genre_list\n",
    "    \n",
    "    def scrape_books_from_genre(self, genre_name: str) -> Tuple[List[str], List[str]]:\n",
    "        if not isinstance(genre_name, str):\n",
    "            raise TypeError(\"genre_name must be a string\")\n",
    "        \n",
    "        genre_name = genre_name.lower()\n",
    "        genre_list = list(map(lambda x: x.lower(), self.genre_list()))\n",
    "\n",
    "        if genre_name not in [genre for genre in genre_list]:\n",
    "            raise ValueError(\"genre_name not present\")\n",
    "        \n",
    "        book_list = []\n",
    "        book_href = []\n",
    "        genre_index = genre_list.index(genre_name) + 2\n",
    "\n",
    "        base_url = BookScraping.base_url + f\"catalogue/category/books/{genre_name}_{genre_index}/index.html\"\n",
    "        url = base_url\n",
    "        page_count = 0\n",
    "\n",
    "        # Scraping a Genre\n",
    "        while url:\n",
    "            page_count += 1\n",
    "\n",
    "            try:\n",
    "                response = self.session.get(url=url, timeout=self.timeout, headers=self.header)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                raise Exception(f\"Error fetching {url}: {e}\")\n",
    "            \n",
    "            print(Fore.GREEN + f\"Scraping page {page_count}...\")\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # All the books in the current page of the genre\n",
    "            books = soup.select(\"article.product_pod\")\n",
    "            book_list.extend([book.h3.select_one(\"a\")[\"title\"] for book in books])\n",
    "            book_href.extend([book.h3.select_one(\"a\")[\"href\"] for book in books])\n",
    "\n",
    "            # Looking for next button in the same genre\n",
    "            next_button = soup.find(\"li\", class_=\"next\")\n",
    "\n",
    "            if next_button:\n",
    "                next_href = next_button.find(\"a\")[\"href\"]\n",
    "                # url = base_url + next_href\n",
    "                url = base_url.replace(\"index.html\", next_href)\n",
    "                time.sleep(random.uniform(self.delay[0], self.delay[1]))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return (book_list, book_href)\n",
    "    \n",
    "    def scrape_book_info(self, book_href: str) -> Dict[str, str]:\n",
    "        if not isinstance(book_href, str):\n",
    "            raise TypeError(\"book_href must be a string\")\n",
    "        \n",
    "        url = BookScraping.base_url + \"catalogue/\" + book_href[9:]\n",
    "        try:\n",
    "            response = self.session.get(url=url, timeout=self.timeout, headers=self.header)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"Error fetching {url}: {e}\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        availability = soup.select_one(\"p\", class_=\"instock availability\").get_text(strip=True)      # availability\n",
    "        price = soup.select_one(\"p.price_color\").get_text(strip=True)       # price\n",
    "        rating_text = soup.select_one(\"p.star-rating\")[\"class\"][-1].lower()\n",
    "        rating = BookScraping.rating_map[rating_text]\n",
    "\n",
    "        table = soup.find(\"table\", class_=\"table table-striped\")\n",
    "        rows = table.select(\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            if row.select_one(\"th\").get_text(strip=True) == 'UPC':\n",
    "                upc = row.select_one(\"td\").get_text(strip=True)     # UPC\n",
    "\n",
    "        book_info = {\"UPC\": upc, \"Price\": price, \"Rating\": rating, \"Availability\": availability, \"URL\": url}     # Recording data\n",
    "        return book_info\n",
    "        \n",
    "    def scrape_all_books(self) -> Tuple[List[str], List[str]]:\n",
    "        url = BookScraping.base_url\n",
    "        book_list = []\n",
    "        book_href = []\n",
    "        page_count = 0\n",
    "\n",
    "        while url:\n",
    "            page_count += 1\n",
    "\n",
    "            try:\n",
    "                response = self.session.get(url=url, timeout=self.timeout, headers=self.header)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                raise Exception(f\"Error fetching {url}: {e}\")\n",
    "            \n",
    "            print(Fore.GREEN + f\"Scraping page {page_count}...\")\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            books = soup.select(\"article.product_pod\")\n",
    "            book_list.extend([book.h3.select_one(\"a\")[\"title\"] for book in books])\n",
    "            book_href.extend([book.h3.select_one(\"a\")[\"href\"] for book in books])\n",
    "\n",
    "            # Looking for next button in the same genre\n",
    "            next_button = soup.find(\"li\", class_=\"next\")\n",
    "\n",
    "            if next_button:\n",
    "                next_href = next_button.find(\"a\")[\"href\"]\n",
    "\n",
    "                if page_count == 1:\n",
    "                    url = url + next_href\n",
    "                else:\n",
    "                    url = BookScraping.base_url + \"catalogue/\" + next_href\n",
    "\n",
    "                time.sleep(random.uniform(self.delay[0], self.delay[1]))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return (book_list, book_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eaae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = QuoteScraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.scrape_author_quotes(\"albert einstein\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.scrape_author_info(\"albert einstein\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb2784",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.scrape_all_authors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1813ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.scrape_all_quotes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbfefc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = BookScraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39db9b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Travel',\n",
       " 'Mystery',\n",
       " 'Historical Fiction',\n",
       " 'Sequential Art',\n",
       " 'Classics',\n",
       " 'Philosophy',\n",
       " 'Romance',\n",
       " 'Womens Fiction',\n",
       " 'Fiction',\n",
       " 'Childrens',\n",
       " 'Religion',\n",
       " 'Nonfiction',\n",
       " 'Music',\n",
       " 'Default',\n",
       " 'Science Fiction',\n",
       " 'Sports and Games',\n",
       " 'Add a comment',\n",
       " 'Fantasy',\n",
       " 'New Adult',\n",
       " 'Young Adult',\n",
       " 'Science',\n",
       " 'Poetry',\n",
       " 'Paranormal',\n",
       " 'Art',\n",
       " 'Psychology',\n",
       " 'Autobiography',\n",
       " 'Parenting',\n",
       " 'Adult Fiction',\n",
       " 'Humor',\n",
       " 'Horror',\n",
       " 'History',\n",
       " 'Food and Drink',\n",
       " 'Christian Fiction',\n",
       " 'Business',\n",
       " 'Biography',\n",
       " 'Thriller',\n",
       " 'Contemporary',\n",
       " 'Spirituality',\n",
       " 'Academic',\n",
       " 'Self Help',\n",
       " 'Historical',\n",
       " 'Christian',\n",
       " 'Suspense',\n",
       " 'Short Stories',\n",
       " 'Novels',\n",
       " 'Health',\n",
       " 'Politics',\n",
       " 'Cultural',\n",
       " 'Erotica',\n",
       " 'Crime']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.genre_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b55a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Chase Me (Paris Nights #2)',\n",
       "  'Black Dust',\n",
       "  'Her Backup Boyfriend (The Sorensen Family #1)',\n",
       "  'First and First (Five Boroughs #3)',\n",
       "  'Fifty Shades Darker (Fifty Shades #2)',\n",
       "  'The Wedding Dress',\n",
       "  'Suddenly in Love (Lake Haven #1)',\n",
       "  'Something More Than This',\n",
       "  'Doing It Over (Most Likely To #1)',\n",
       "  \"The Wedding Pact (The O'Malleys #2)\",\n",
       "  'Hold Your Breath (Search and Rescue #1)',\n",
       "  'Dirty (Dive Bar #1)',\n",
       "  'Take Me Home Tonight (Rock Star Romance #3)',\n",
       "  'Off the Hook (Fishing for Trouble #1)',\n",
       "  \"A Gentleman's Position (Society of Gentlemen #3)\",\n",
       "  'Sit, Stay, Love',\n",
       "  \"A Girl's Guide to Moving On (New Beginnings #2)\",\n",
       "  'The Perfect Play (Play by Play #1)',\n",
       "  'Dark Lover (Black Dagger Brotherhood #1)',\n",
       "  'Changing the Game (Play by Play #2)',\n",
       "  'A Walk to Remember',\n",
       "  'The Purest Hook (Second Circle Tattoos #3)',\n",
       "  'The Obsession',\n",
       "  'Reservations for Two',\n",
       "  \"Best of My Love (Fool's Gold #20)\",\n",
       "  'Where Lightning Strikes (Bleeding Stars #3)',\n",
       "  'This One Moment (Pushing Limits #1)',\n",
       "  'Rhythm, Chord & Malykhin',\n",
       "  'My Perfect Mistake (Over the Top #1)',\n",
       "  'Listen to Me (Fusion #1)',\n",
       "  'Imperfect Harmony',\n",
       "  'Fighting Fate (Fighting #6)',\n",
       "  'Deep Under (Walker Security #1)',\n",
       "  \"Charity's Cross (Charles Towne Belles #4)\",\n",
       "  'Bounty (Colorado Mountain #7)'],\n",
       " ['../../../chase-me-paris-nights-2_977/index.html',\n",
       "  '../../../black-dust_976/index.html',\n",
       "  '../../../her-backup-boyfriend-the-sorensen-family-1_896/index.html',\n",
       "  '../../../first-and-first-five-boroughs-3_893/index.html',\n",
       "  '../../../fifty-shades-darker-fifty-shades-2_892/index.html',\n",
       "  '../../../the-wedding-dress_864/index.html',\n",
       "  '../../../suddenly-in-love-lake-haven-1_835/index.html',\n",
       "  '../../../something-more-than-this_834/index.html',\n",
       "  '../../../doing-it-over-most-likely-to-1_802/index.html',\n",
       "  '../../../the-wedding-pact-the-omalleys-2_767/index.html',\n",
       "  '../../../hold-your-breath-search-and-rescue-1_700/index.html',\n",
       "  '../../../dirty-dive-bar-1_615/index.html',\n",
       "  '../../../take-me-home-tonight-rock-star-romance-3_605/index.html',\n",
       "  '../../../off-the-hook-fishing-for-trouble-1_601/index.html',\n",
       "  '../../../a-gentlemans-position-society-of-gentlemen-3_584/index.html',\n",
       "  '../../../sit-stay-love_486/index.html',\n",
       "  '../../../a-girls-guide-to-moving-on-new-beginnings-2_359/index.html',\n",
       "  '../../../the-perfect-play-play-by-play-1_352/index.html',\n",
       "  '../../../dark-lover-black-dagger-brotherhood-1_319/index.html',\n",
       "  '../../../changing-the-game-play-by-play-2_317/index.html',\n",
       "  '../../../a-walk-to-remember_312/index.html',\n",
       "  '../../../the-purest-hook-second-circle-tattoos-3_271/index.html',\n",
       "  '../../../the-obsession_268/index.html',\n",
       "  '../../../reservations-for-two_211/index.html',\n",
       "  '../../../best-of-my-love-fools-gold-20_127/index.html',\n",
       "  '../../../where-lightning-strikes-bleeding-stars-3_96/index.html',\n",
       "  '../../../this-one-moment-pushing-limits-1_88/index.html',\n",
       "  '../../../rhythm-chord-malykhin_47/index.html',\n",
       "  '../../../my-perfect-mistake-over-the-top-1_35/index.html',\n",
       "  '../../../listen-to-me-fusion-1_29/index.html',\n",
       "  '../../../imperfect-harmony_26/index.html',\n",
       "  '../../../fighting-fate-fighting-6_18/index.html',\n",
       "  '../../../deep-under-walker-security-1_15/index.html',\n",
       "  '../../../charitys-cross-charles-towne-belles-4_12/index.html',\n",
       "  '../../../bounty-colorado-mountain-7_9/index.html'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.scrape_books_from_genre(\"romance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.scrape_all_books()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a196c3c",
   "metadata": {},
   "source": [
    "Scraping Multiple Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b53824",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(lambda: defaultdict(list))       # Quote data is stored here\n",
    "author_details = defaultdict()      # Author details are stored here\n",
    "page_count = 0\n",
    "\n",
    "while url:\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=header)     # Getting response from website\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    quotes = soup.find_all(\"div\", class_=\"quote\")       # Find all quotes in 1 page\n",
    "    \n",
    "    page_count += 1\n",
    "    print(page_count)\n",
    "\n",
    "    for quote in quotes:\n",
    "        text = quote.find(\"span\", class_=\"text\").get_text(strip=True)       # quote_text\n",
    "        author = quote.find(\"small\", class_=\"author\").get_text(strip=True)      # author\n",
    "        tags = [tag.get_text(strip=True) for tag in quote.find_all(\"a\", class_=\"tag\")]      # tags associated with the quote\n",
    "\n",
    "        for tag in tags:\n",
    "            data[author][tag].append(text)      # Listing all quotes by author and tag\n",
    "\n",
    "        if author not in author_details:        # Scraping author details if not scraped\n",
    "            print(author)\n",
    "            about_href = quote.find(\"a\")[\"href\"]\n",
    "            author_url = base_url + about_href\n",
    "\n",
    "            author_response = requests.get(author_url)\n",
    "            author_soup = BeautifulSoup(author_response.text, \"html.parser\")\n",
    "\n",
    "            born_date = author_soup.find(\"span\", class_=\"author-born-date\").get_text(strip=True)\n",
    "            born_location = author_soup.find(\"span\", class_=\"author-born-location\").get_text(strip=True)[3:]\n",
    "\n",
    "            author_details[author] = {\"Born On\": born_date, \"Location\": born_location}\n",
    "            time.sleep(random.uniform(1, 3))        # Delay requests to reduce traffic on website\n",
    "            \n",
    "    next_button = soup.find(\"li\", class_=\"next\")        # Next button at the end of page for author_details\n",
    "\n",
    "    if next_button:     # If next_buuton is availbale\n",
    "        next_href = next_button.find(\"a\")[\"href\"]\n",
    "        url = base_url + next_href      # url for next page\n",
    "    else:\n",
    "        url = None\n",
    "    \n",
    "    print()\n",
    "    time.sleep(random.uniform(1, 3))        # Delay requests to reduce traffic on website for next_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579cccd",
   "metadata": {},
   "source": [
    "Writing data to JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c1fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"quotes.json\", mode=\"w\", encoding='utf-8') as q:\n",
    "    json.dump(data,  q, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(\"author_details.json\", mode=\"w\", encoding='utf-8') as a:\n",
    "    json.dump(author_details, a, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ef96c",
   "metadata": {},
   "source": [
    "SCRAPING BOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f21287b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://books.toscrape.com/\"\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36\"}      # Chrome browser string\n",
    "response = requests.get(base_url, timeout=5, headers=header)\n",
    "print(response.status_code)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "side_panel = soup.find(\"ul\", class_=\"nav nav-list\")\n",
    "genres = side_panel.select(\"ul ul li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e3d6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_books_from_genre(genre_url):\n",
    "    global rating_map, page_count, book_data\n",
    "    base_genre_url = genre_url\n",
    "\n",
    "    # Scraping a Genre\n",
    "    while genre_url:\n",
    "        page_count += 1\n",
    "        print(page_count)\n",
    "\n",
    "        genre_response = requests.get(genre_url, timeout=5, headers=header)\n",
    "        genre_soup = BeautifulSoup(genre_response.text, \"html.parser\")\n",
    "\n",
    "        # All the books in the current page of the genre\n",
    "        books = genre_soup.select(\"article.product_pod\")\n",
    "\n",
    "        # Scraping details of each book\n",
    "        for book in books:\n",
    "            price = book.select_one(\"p.price_color\").get_text(strip=True)       # price\n",
    "            rating_text = book.select_one(\"p.star-rating\")[\"class\"][-1].lower()\n",
    "            rating = rating_map[rating_text]        # rating\n",
    "\n",
    "            # Scraping details from individual book pages\n",
    "            book_href = book.h3.select_one(\"a\")[\"href\"]\n",
    "            book_url = base_url + \"catalogue/\" + book_href[9:]\n",
    "            book_response = requests.get(book_url, timeout=5, headers=header)\n",
    "            book_soup = BeautifulSoup(book_response.text, \"html.parser\")\n",
    "\n",
    "            title = book_soup.h1.get_text(strip=True)       # title\n",
    "            print(title)\n",
    "            availability = book_soup.find(\"p\", class_=\"instock availability\").get_text(strip=True)      # availability\n",
    "            table = book_soup.find(\"table\", class_=\"table table-striped\")\n",
    "            rows = table.select(\"tr\")\n",
    "\n",
    "            for row in rows:\n",
    "                if row.select_one(\"th\").get_text(strip=True) == 'UPC':\n",
    "                    upc = row.select_one(\"td\").get_text(strip=True)     # UPC\n",
    "\n",
    "            book_data[genre_text][title] = {\"UPC\": upc, \"Price\": price, \"Rating\": rating, \"Availability\": availability}     # Recording data\n",
    "            # time.sleep(random.uniform(1,2))\n",
    "\n",
    "        # Looking for next button in the same genre\n",
    "        next_button = genre_soup.find(\"li\", class_=\"next\")\n",
    "        print()\n",
    "\n",
    "        if next_button:\n",
    "            next_href = next_button.find(\"a\")[\"href\"]\n",
    "            genre_url = base_genre_url.replace(\"index.html\", next_href)\n",
    "            # time.sleep(random.uniform(1, 2))\n",
    "        else:\n",
    "            page_count = 0\n",
    "            print()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b13541",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_count = 0\n",
    "rating_map = {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5}\n",
    "book_data = defaultdict(lambda: defaultdict())\n",
    "\n",
    "for genre in genres:\n",
    "    genre_text = genre.get_text(strip=True)\n",
    "    genre_href = genre.find(\"a\")[\"href\"]\n",
    "    print(genre_text)\n",
    "\n",
    "    genre_url = base_url + genre_href\n",
    "    scrape_books_from_genre(genre_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5b7c203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/category/books/mystery_3/index.html\n"
     ]
    }
   ],
   "source": [
    "genre = genres[1]\n",
    "\n",
    "genre_href = genre.find(\"a\")[\"href\"]\n",
    "genre_url = base_url + genre_href\n",
    "print(genre_url)\n",
    "\n",
    "genre_response = requests.get(genre_url, timeout=5, headers=header)\n",
    "genre_soup = BeautifulSoup(genre_response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd906f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/sharp-objects_997/index.html'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = genre_soup.select(\"article.product_pod\")\n",
    "book = books[0]\n",
    "book_href = book.h3.select_one(\"a\")[\"href\"]\n",
    "book_url = base_url + \"catalogue/\" + book_href[9:]\n",
    "book_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4e6973b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li class=\"next\"><a href=\"page-2.html\">next</a></li>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_button = genre_soup.find(\"li\", class_=\"next\")\n",
    "print(next_button)\n",
    "next_href = next_button.find(\"a\")[\"href\"]\n",
    "genre_url = genre_url.replace(\"index.html\", next_href)\n",
    "genre_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f8a6bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['star-rating', 'Two']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = book.select_one(\"p.star-rating\")\n",
    "rating[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f50d89c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n  \\n\\n\\n    It\\'s Only the Himalayas | Books to Scrape - Sandbox\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBooks to Scrape We love being scraped!\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\nBooks\\n\\n\\nTravel\\n\\nIt\\'s Only the Himalayas\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIt\\'s Only the Himalayas\\nÃ‚Â£45.17\\n\\n\\n    \\n        In stock (19 available)\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nWarning! This is a demo website for web scraping purposes. Prices and ratings here were randomly assigned and have no real meaning.\\n\\n\\n\\nProduct Description\\n\\nÃ¢\\x80\\x9cWherever you go, whatever you do, just . . . donÃ¢\\x80\\x99t do anything stupid.Ã¢\\x80\\x9d Ã¢\\x80\\x94My MotherDuring her yearlong adventure backpacking from South Africa to Singapore, S. Bedford definitely did a few things her mother might classify as \"stupid.\" She swam with great white sharks in South Africa, ran from lions in Zimbabwe, climbed a Himalayan mountain without training in Nepal, and wa Ã¢\\x80\\x9cWherever you go, whatever you do, just . . . donÃ¢\\x80\\x99t do anything stupid.Ã¢\\x80\\x9d Ã¢\\x80\\x94My MotherDuring her yearlong adventure backpacking from South Africa to Singapore, S. Bedford definitely did a few things her mother might classify as \"stupid.\" She swam with great white sharks in South Africa, ran from lions in Zimbabwe, climbed a Himalayan mountain without training in Nepal, and watched as her friend was attacked by a monkey in Indonesia.But interspersed in those slightly more crazy moments, Sue Bedfored and her friend \"Sara the Stoic\" experienced the sights, sounds, life, and culture of fifteen countries. Joined along the way by a few friends and their aging fathers here and there, Sue and Sara experience the trip of a lifetime. They fall in love with the world, cultivate an appreciation for home, and discover who, or what, they want to become.It\\'s Only the Himalayas is the incredibly funny, sometimes outlandish, always entertaining confession of a young backpacker that will inspire you to take your own adventure. ...more\\n\\nProduct Information\\n\\n\\n\\nUPCa22124811bfa8350\\n\\n\\nProduct TypeBooks\\n\\n\\nPrice (excl. tax)Ã‚Â£45.17\\n\\n\\nPrice (incl. tax)Ã‚Â£45.17\\n\\n\\nTaxÃ‚Â£0.00\\n\\n\\nAvailability\\nIn stock (19 available)\\n\\n\\nNumber of reviews\\n0\\n\\n\\n\\n\\n\\n\\n\\nProducts you recently viewed\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLibertarianism for Beginners\\n\\nÃ‚Â£51.33\\n\\n\\n    \\n        In stock\\n    \\n\\n\\nAdd to basket\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMesaerion: The Best Science ...\\n\\nÃ‚Â£37.59\\n\\n\\n    \\n        In stock\\n    \\n\\n\\nAdd to basket\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOlio\\n\\nÃ‚Â£23.88\\n\\n\\n    \\n        In stock\\n    \\n\\n\\nAdd to basket\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOur Band Could Be ...\\n\\nÃ‚Â£57.25\\n\\n\\n    \\n        In stock\\n    \\n\\n\\nAdd to basket\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRip it Up and ...\\n\\nÃ‚Â£35.02\\n\\n\\n    \\n        In stock\\n    \\n\\n\\nAdd to basket\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nScott Pilgrim\\'s Precious Little ...\\n\\nÃ‚Â£52.29\\n\\n\\n    \\n        In stock\\n    \\n\\n\\nAdd to basket\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_href = book.h3.select_one(\"a\")[\"href\"]\n",
    "book_url = base_url + \"catalogue/\" + book_href[9:]\n",
    "print(book_url)\n",
    "book_response = requests.get(book_url, timeout=5, headers=header)\n",
    "book_soup = BeautifulSoup(book_response.text, \"html.parser\")\n",
    "\n",
    "book_soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8e86c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UPC'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = book_soup.select_one(\"table.table.table-striped\")\n",
    "rows = table.select(\"tr\")\n",
    "row = rows[0]\n",
    "row.select_one(\"th\").get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb2304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arya', 12]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"Name\": \"Arya\", \"Class\": 12}\n",
    "list(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252f75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
